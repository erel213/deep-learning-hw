{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SE6qcw_j8Pi2"
      },
      "source": [
        "In this homework assignment, you are requested to implement a full backprop algorithm using only *numpy*.\n",
        "\n",
        "- We assume sigmoid activation across all layers.\n",
        "- We assume a single value in the output layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "UV4RvXYL8k85"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "np.random.seed(42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SRml6glFIPCa"
      },
      "source": [
        "The following class represents a simple feed forward network with multiple layers. The network class provides methods for running forward and backward for a single instance, throught the network. You should implement the methods (indicated with TODO), that performs forward and backward for an entire batch. Note, the idea is to use matrix multiplications, and not running standard loops over the instances in the batch."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "kLdNoCt58qg5"
      },
      "outputs": [],
      "source": [
        "np.random.seed(42)\n",
        "import matplotlib.pyplot as plt\n",
        "from typing import List, Dict, Tuple\n",
        "from utils import MyNN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "Qib6W4QXO644"
      },
      "outputs": [],
      "source": [
        "nn = MyNN(0.01, [3, 2, 1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "4nQR8QllPf_5"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'W_1': array([[ 0.04967142, -0.01382643,  0.06476885],\n",
              "        [ 0.15230299, -0.02341534, -0.0234137 ]]),\n",
              " 'b_1': array([0.15792128, 0.07674347]),\n",
              " 'W_2': array([[-0.04694744,  0.054256  ]]),\n",
              " 'b_2': array([-0.04634177])}"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "nn.model_params"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "VXiyn-yrPC6-"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[0.48946]\n"
          ]
        }
      ],
      "source": [
        "x = np.random.randn(3)\n",
        "y = np.random.randn(1)\n",
        "\n",
        "y_hat = nn.forward_single_instance(x)\n",
        "print(y_hat)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "k5M50i3plclj"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2\n",
            "1\n"
          ]
        }
      ],
      "source": [
        "nn.backward_single_instance(y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "CWnZB1YmYnIt"
      },
      "outputs": [],
      "source": [
        "def train(X, y, epochs, batch_size):\n",
        "  '''\n",
        "  Train procedure, please note the TODOs inside\n",
        "  '''\n",
        "  for e in range(1, epochs + 1):\n",
        "    epoch_loss = 0\n",
        "    # TODO: shuffle\n",
        "    batches = np.random.permutation(X.shape[1])\n",
        "    #... TODO: divide to batches\n",
        "    batch_indices = np.array_split(batches, X.shape[1] // batch_size)\n",
        "    \n",
        "    for batch_idx in batch_indices:\n",
        "        # Use indices to select the corresponding data\n",
        "        X_b = X[:, batch_idx]\n",
        "        y_b = y[:, batch_idx]\n",
        "        \n",
        "        y_hat = nn.forward_batch(X_b)\n",
        "        epoch_loss += nn.log_loss_batch(y_hat, y_b)\n",
        "        nn.backward_batch(y_b)\n",
        "        nn.update()\n",
        "    print(f'Epoch {e}, loss={epoch_loss/len(batch_indices)}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "cE1ydWlatkty"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'nn' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[19]\u001b[39m\u001b[32m, line 10\u001b[39m\n\u001b[32m      7\u001b[39m batch_size = \u001b[32m8\u001b[39m\n\u001b[32m      8\u001b[39m epochs = \u001b[32m2\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/projects/msc-computer-science/deep-learning-hw/homework1/utils.py:238\u001b[39m, in \u001b[36mtrain\u001b[39m\u001b[34m(X, y, epochs, batch_size)\u001b[39m\n\u001b[32m    235\u001b[39m X_b = X[:, batch_idx]\n\u001b[32m    236\u001b[39m y_b = y[:, batch_idx]\n\u001b[32m--> \u001b[39m\u001b[32m238\u001b[39m y_hat = \u001b[43mnn\u001b[49m.forward_batch(X_b)\n\u001b[32m    239\u001b[39m epoch_loss += nn.log_loss_batch(y_hat, y_b)\n\u001b[32m    240\u001b[39m nn.backward_batch(y_b)\n",
            "\u001b[31mNameError\u001b[39m: name 'nn' is not defined"
          ]
        }
      ],
      "source": [
        "# TODO: Make sure the following network trains properly\n",
        "\n",
        "nn = MyNN(0.001, [6, 4, 3, 1])\n",
        "\n",
        "X = np.random.randn(6, 100)\n",
        "y = np.random.randn(1, 100)\n",
        "batch_size = 8\n",
        "epochs = 2\n",
        "\n",
        "train(X, y, epochs, batch_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6dY4scUksulC"
      },
      "source": [
        "#TODO: train on an external dataset\n",
        "\n",
        "Train on the *hour.csv* file with a split of 75% training 10% validation and 15% for test.\n",
        "Use the following features from the data:\n",
        "\n",
        "* temp\n",
        "* atemp\n",
        "* hum\n",
        "* windspeed\n",
        "* weekday\n",
        "\n",
        "The response variable is, *success*\n",
        "\n",
        "The architecture of the network should be: [5, 40, 30, 10, 7, 5, 3, 1].\n",
        "\n",
        "Use batch_size=8, and train it for 100 epochs on the train set (based on the split as requested above).\n",
        "\n",
        "Then, plot train and validation loss per epoch."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "3\n",
            "2\n",
            "1\n",
            "3\n",
            "2\n",
            "1\n",
            "3\n",
            "2\n",
            "1\n",
            "3\n",
            "2\n",
            "1\n",
            "3\n",
            "2\n",
            "1\n",
            "3\n",
            "2\n",
            "1\n",
            "3\n",
            "2\n",
            "1\n",
            "3\n",
            "2\n",
            "1\n",
            "3\n",
            "2\n",
            "1\n"
          ]
        },
        {
          "ename": "ValueError",
          "evalue": "operands could not be broadcast together with shapes (4,) (6,) ",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m\n",
            "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 10\u001b[39m\n",
            "\u001b[32m      7\u001b[39m batch_size = \u001b[32m8\u001b[39m\n",
            "\u001b[32m      8\u001b[39m epochs = \u001b[32m2\u001b[39m\n",
            "\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m)\u001b[49m\n",
            "\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 20\u001b[39m, in \u001b[36mtrain\u001b[39m\u001b[34m(X, y, epochs, batch_size)\u001b[39m\n",
            "\u001b[32m     18\u001b[39m     epoch_loss += nn.log_loss_batch(y_hat, y_b)\n",
            "\u001b[32m     19\u001b[39m     nn.backward_batch(y_b)\n",
            "\u001b[32m---> \u001b[39m\u001b[32m20\u001b[39m     \u001b[43mnn\u001b[49m\u001b[43m.\u001b[49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[32m     21\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, loss=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch_loss/\u001b[38;5;28mlen\u001b[39m(batch_indices)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m)\n",
            "\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 118\u001b[39m, in \u001b[36mMyNN.update\u001b[39m\u001b[34m(self)\u001b[39m\n",
            "\u001b[32m    116\u001b[39m \u001b[38;5;66;03m# Update weights and biases using gradient descent\u001b[39;00m\n",
            "\u001b[32m    117\u001b[39m \u001b[38;5;28mself\u001b[39m.model_params[\u001b[33m'\u001b[39m\u001b[33mW_\u001b[39m\u001b[33m'\u001b[39m + \u001b[38;5;28mstr\u001b[39m(layer_index)] = W - \u001b[38;5;28mself\u001b[39m.learning_rate * dW\n",
            "\u001b[32m--> \u001b[39m\u001b[32m118\u001b[39m \u001b[38;5;28mself\u001b[39m.model_params[\u001b[33m'\u001b[39m\u001b[33mb_\u001b[39m\u001b[33m'\u001b[39m + \u001b[38;5;28mstr\u001b[39m(layer_index)] = \u001b[43mb\u001b[49m\u001b[43m \u001b[49m\u001b[43m-\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m \u001b[49m\u001b[43mdb\u001b[49m\n",
            "\n",
            "\u001b[31mValueError\u001b[39m: operands could not be broadcast together with shapes (4,) (6,) "
          ]
        }
      ],
      "source": [
        "# TODO: Make sure the following network trains properly\n",
        "\n",
        "nn = MyNN(0.001, [6, 4, 3, 1])\n",
        "\n",
        "X = np.random.randn(6, 100)\n",
        "y = np.random.randn(1, 100)\n",
        "batch_size = 8\n",
        "epochs = 2\n",
        "\n",
        "train(X, y, epochs, batch_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZKAxdO2I1IGT"
      },
      "source": [
        "##  your code goes here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CHGoWEJk1K7r"
      },
      "source": [
        "###  Data Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e4Ra8vc_1NCE"
      },
      "outputs": [],
      "source": [
        "class DataLoader:\n",
        "  def __init__(self, X, y, batch_size):\n",
        "    self.X = X\n",
        "    self.y = y\n",
        "    self.batch_size = batch_size\n",
        "    self.indices = np.random.permutation(X.shape[1])\n",
        "    self.batches = np.array_split(self.indices, self.batch_size)\n",
        "\n",
        "  def __iter__(self):\n",
        "    return self\n",
        "  \n",
        "  def __next__(self):\n",
        "    if len(self.batches) == 0:\n",
        "      raise StopIteration\n",
        "    batch_indices = self.batches.pop(0)\n",
        "    X_batch = self.X[:, batch_indices]\n",
        "    y_batch = self.y[batch_indices]\n",
        "    return X_batch, y_batch\n",
        "\n",
        "def preprocess_data(file_path):\n",
        "  '''\n",
        "  Preprocess the bike sharing dataset ('hour.csv')\n",
        "  '''\n",
        "  # Load numpy arrays\n",
        "  with open(file_path, 'r') as file:\n",
        "    data = np.loadtxt(file, delimiter=',', skiprows=1)  # skiprows=1 to skip the header row\n",
        "    # Select required features: temp, atemp, hum, windspeed, weekday\n",
        "    X = data[:, [10, 11, 12, 13, 7]]  # indices for temp, atemp, hum, windspeed, weekday\n",
        "    y = data[:, 17]  # index for success\n",
        "    # Normalize/standardize features\n",
        "    X = (X - X.mean(axis=0)) / X.std(axis=0)\n",
        "    # Split data into training, validation, and test sets\n",
        "    train_X, train_y, val_X, val_y, test_X, test_y = split_data(X, y)\n",
        "    # Create DataLoader objects\n",
        "    train_loader = DataLoader(train_X, train_y, batch_size=8)\n",
        "    val_loader = DataLoader(val_X, val_y, batch_size=8)\n",
        "    test_loader = DataLoader(test_X, test_y, batch_size=8)\n",
        "    return train_loader, val_loader, test_loader\n",
        "\n",
        "\n",
        "def split_data(X, y, train_ratio=0.75, val_ratio=0.1, test_ratio=0.15):\n",
        "  '''\n",
        "  Split the data into training, validation, and test sets\n",
        "  X - numpy array of shape (num_instances, num_features)\n",
        "  y - numpy array of shape (num_instances,)\n",
        "  train_ratio - float, the ratio of the training set\n",
        "  val_ratio - float, the ratio of the validation set\n",
        "  test_ratio - float, the ratio of the test set\n",
        "  Returns:\n",
        "  train_X, train_y, val_X, val_y, test_X, test_y\n",
        "  '''\n",
        "  # Calculate the number of instances for each set\n",
        "  num_instances = X.shape[0]\n",
        "  num_train = int(train_ratio * num_instances)\n",
        "  num_val = int(val_ratio * num_instances)\n",
        "\n",
        "  # Shuffle the data\n",
        "  indices = np.random.permutation(num_instances)\n",
        "  X = X[indices]\n",
        "  y = y[indices]\n",
        "\n",
        "  # Split the data\n",
        "  train_X = X[:num_train]\n",
        "  train_y = y[:num_train]\n",
        "  val_X = X[num_train:num_train+num_val]\n",
        "  val_y = y[num_train:num_train+num_val]\n",
        "  test_X = X[num_train+num_val:]\n",
        "  test_y = y[num_train+num_val:]\n",
        "\n",
        "  return train_X, train_y, val_X, val_y, test_X, test_y"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "glp6Vz3B1TZo"
      },
      "source": [
        "### Model Training\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO: Create visualizations of the learning process\n",
        "# - Plot the training loss per epoch\n",
        "# - Create additional relevant plots (validation loss, learning curves, etc.)\n",
        "# - Make sure all plots have proper labels, titles, and legends\n",
        "# - Add brief analysis of what the plots reveal about your model's performances\n",
        "\n",
        "class TrainingVisualizer:\n",
        "    def __init__(self):\n",
        "        \"\"\"Initialize the training visualizer with empty history.\"\"\"\n",
        "        self.history = {\n",
        "            'train_loss': [],\n",
        "            'val_loss': [],\n",
        "            'train_accuracy': [],\n",
        "            'val_accuracy': [],\n",
        "            'learning_rates': []\n",
        "        }\n",
        "    \n",
        "    def update(self, \n",
        "              val_loss: float, \n",
        "              train_loss: float, \n",
        "              train_accuracy: float = None, \n",
        "              val_accuracy: float = None,\n",
        "              learning_rate: float = None):\n",
        "        \"\"\"Update the training history with new metrics.\"\"\"\n",
        "        self.history['train_loss'].append(train_loss)\n",
        "        self.history['val_loss'].append(val_loss)\n",
        "        if train_accuracy is not None:\n",
        "            self.history['train_accuracy'].append(train_accuracy)\n",
        "        if val_accuracy is not None:\n",
        "            self.history['val_accuracy'].append(val_accuracy)\n",
        "        if learning_rate is not None:\n",
        "            self.history['learning_rates'].append(learning_rate)\n",
        "    \n",
        "    def plot_training_curves(self, save_path: str = None):\n",
        "        \"\"\"Plot training and validation loss curves.\"\"\"\n",
        "        plt.figure(figsize=(12, 5))\n",
        "        \n",
        "        # Plot loss curves\n",
        "        plt.subplot(1, 2, 1)\n",
        "        plt.plot(self.history['train_loss'], label='Training Loss', color='blue')\n",
        "        plt.plot(self.history['val_loss'], label='Validation Loss', color='red')\n",
        "        plt.title('Training and Validation Loss')\n",
        "        plt.xlabel('Epoch')\n",
        "        plt.ylabel('Loss')\n",
        "        plt.legend()\n",
        "        plt.grid(True)\n",
        "        \n",
        "        # Plot accuracy curves if available\n",
        "        if self.history['train_accuracy'] and self.history['val_accuracy']:\n",
        "            plt.subplot(1, 2, 2)\n",
        "            plt.plot(self.history['train_accuracy'], label='Training Accuracy', color='blue')\n",
        "            plt.plot(self.history['val_accuracy'], label='Validation Accuracy', color='red')\n",
        "            plt.title('Training and Validation Accuracy')\n",
        "            plt.xlabel('Epoch')\n",
        "            plt.ylabel('Accuracy')\n",
        "            plt.legend()\n",
        "            plt.grid(True)\n",
        "        \n",
        "        plt.tight_layout()\n",
        "        if save_path:\n",
        "            plt.savefig(save_path)\n",
        "        plt.show()\n",
        "    \n",
        "    def plot_learning_rate(self, save_path: str = None):\n",
        "        \"\"\"Plot learning rate changes over time.\"\"\"\n",
        "        if not self.history['learning_rates']:\n",
        "            return\n",
        "        \n",
        "        plt.figure(figsize=(8, 4))\n",
        "        plt.plot(self.history['learning_rates'], color='green')\n",
        "        plt.title('Learning Rate Changes')\n",
        "        plt.xlabel('Epoch')\n",
        "        plt.ylabel('Learning Rate')\n",
        "        plt.grid(True)\n",
        "        \n",
        "        if save_path:\n",
        "            plt.savefig(save_path)\n",
        "        plt.show()\n",
        "    \n",
        "    def plot_loss_distribution(self, save_path: str = None):\n",
        "        \"\"\"Plot the distribution of training and validation losses.\"\"\"\n",
        "        plt.figure(figsize=(10, 4))\n",
        "        \n",
        "        # Plot histograms\n",
        "        plt.subplot(1, 2, 1)\n",
        "        plt.hist(self.history['train_loss'], bins=30, alpha=0.5, label='Training Loss', color='blue')\n",
        "        plt.hist(self.history['val_loss'], bins=30, alpha=0.5, label='Validation Loss', color='red')\n",
        "        plt.title('Loss Distribution')\n",
        "        plt.xlabel('Loss Value')\n",
        "        plt.ylabel('Frequency')\n",
        "        plt.legend()\n",
        "        \n",
        "        # Plot box plots\n",
        "        plt.subplot(1, 2, 2)\n",
        "        plt.boxplot([self.history['train_loss'], self.history['val_loss']], \n",
        "                   labels=['Training Loss', 'Validation Loss'])\n",
        "        plt.title('Loss Box Plot')\n",
        "        plt.ylabel('Loss Value')\n",
        "        \n",
        "        plt.tight_layout()\n",
        "        if save_path:\n",
        "            plt.savefig(save_path)\n",
        "        plt.show()\n",
        "    \n",
        "    def generate_summary(self) -> Dict:\n",
        "        \"\"\"Generate a summary of the training metrics.\"\"\"\n",
        "        summary = {\n",
        "            'final_train_loss': self.history['train_loss'][-1],\n",
        "            'final_val_loss': self.history['val_loss'][-1],\n",
        "            'min_train_loss': min(self.history['train_loss']),\n",
        "            'min_val_loss': min(self.history['val_loss']),\n",
        "            'epochs_trained': len(self.history['train_loss'])\n",
        "        }\n",
        "        \n",
        "        if self.history['train_accuracy'] and self.history['val_accuracy']:\n",
        "            summary.update({\n",
        "                'final_train_accuracy': self.history['train_accuracy'][-1],\n",
        "                'final_val_accuracy': self.history['val_accuracy'][-1],\n",
        "                'max_train_accuracy': max(self.history['train_accuracy']),\n",
        "                'max_val_accuracy': max(self.history['val_accuracy'])\n",
        "            })\n",
        "        \n",
        "        return summary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LXlKl21D1UGn"
      },
      "outputs": [],
      "source": [
        "# TODO: Train the neural network\n",
        "# - Implement the network with architecture [5, 40, 30, 10, 7, 5, 3, 1]\n",
        "# - Train for exactly 100 epochs on the training set\n",
        "# - Use batch_size=8 as specified\n",
        "# - Calculate and store train and validation loss for each epoch\n",
        "# - Track training progres\n",
        "def train_nn(train_loader: DataLoader, val_loader: DataLoader, epochs: int):\n",
        "    \"\"\"\n",
        "    Train the neural network with visualization support.\n",
        "    \n",
        "    Parameters:\n",
        "    train_loader: DataLoader object for the training set\n",
        "    val_loader: DataLoader object for the validation set\n",
        "    epochs: int, the number of epochs to train the network\n",
        "    \"\"\"\n",
        "    nn = MyNN(0.01, [5, 40, 30, 10, 7, 5, 3, 1])\n",
        "    visualizer = TrainingVisualizer()\n",
        "    \n",
        "    for epoch in range(epochs):\n",
        "        # Training phase\n",
        "        train_loss = 0\n",
        "        train_correct = 0\n",
        "        train_total = 0\n",
        "        \n",
        "        for X_batch, y_batch in train_loader:\n",
        "            # Forward pass\n",
        "            y_hat = nn.forward_batch(X_batch)\n",
        "            train_loss += nn.log_loss_batch(y_hat, y_batch)\n",
        "            \n",
        "            # Calculate accuracy\n",
        "            predictions = (y_hat > 0.5).astype(int)\n",
        "            train_correct += np.sum(predictions == y_batch)\n",
        "            train_total += y_batch.size\n",
        "            \n",
        "            # Backward pass and update\n",
        "            nn.backward_batch(y_batch)\n",
        "            nn.update()\n",
        "        \n",
        "        # Calculate average training loss and accuracy\n",
        "        train_loss /= len(train_loader)\n",
        "        train_accuracy = train_correct / train_total\n",
        "        \n",
        "        # Validation phase\n",
        "        val_loss = 0\n",
        "        val_correct = 0\n",
        "        val_total = 0\n",
        "        \n",
        "        for X_batch, y_batch in val_loader:\n",
        "            y_hat = nn.forward_batch(X_batch)\n",
        "            val_loss += nn.log_loss_batch(y_hat, y_batch)\n",
        "            \n",
        "            # Calculate accuracy\n",
        "            predictions = (y_hat > 0.5).astype(int)\n",
        "            val_correct += np.sum(predictions == y_batch)\n",
        "            val_total += y_batch.size\n",
        "        \n",
        "        # Calculate average validation loss and accuracy\n",
        "        val_loss /= len(val_loader)\n",
        "        val_accuracy = val_correct / val_total\n",
        "        \n",
        "        # Update visualizer\n",
        "        visualizer.update(\n",
        "            train_loss=train_loss,\n",
        "            val_loss=val_loss,\n",
        "            train_accuracy=train_accuracy,\n",
        "            val_accuracy=val_accuracy,\n",
        "            learning_rate=nn.learning_rate\n",
        "        )\n",
        "        \n",
        "        # Print progress\n",
        "        print(f'Epoch {epoch+1}/{epochs}:')\n",
        "        print(f'  Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}')\n",
        "        print(f'  Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f}')\n",
        "    \n",
        "    # Generate visualizations\n",
        "    visualizer.plot_training_curves()\n",
        "    visualizer.plot_learning_rate()\n",
        "    visualizer.plot_loss_distribution()\n",
        "    \n",
        "    # Print training summary\n",
        "    summary = visualizer.generate_summary()\n",
        "    print(\"\\nTraining Summary:\")\n",
        "    for metric, value in summary.items():\n",
        "        print(f\"{metric}: {value:.4f}\")\n",
        "    \n",
        "    return nn, visualizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cwBcg1yy1b-j"
      },
      "source": [
        "### Visualization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xQIAgWhe1i_2"
      },
      "source": [
        "### Model Evaluation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IfquqstM1iIO"
      },
      "outputs": [],
      "source": [
        "# TODO: Evaluate model performance on the test set\n",
        "# - Calculate and report the loss on the test set\n",
        "# - Calculate and report the accuracy on the test set\n",
        "# - Compare test performance with training/validation performance\n",
        "# - Analyze model strengths and weaknesses\n",
        "# - Discuss any overfitting/underfitting issues observed"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
