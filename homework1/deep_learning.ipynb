{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SE6qcw_j8Pi2"
      },
      "source": [
        "In this homework assignment, you are requested to implement a full backprop algorithm using only *numpy*.\n",
        "\n",
        "- We assume sigmoid activation across all layers.\n",
        "- We assume a single value in the output layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "UV4RvXYL8k85"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "np.random.seed(42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SRml6glFIPCa"
      },
      "source": [
        "The following class represents a simple feed forward network with multiple layers. The network class provides methods for running forward and backward for a single instance, throught the network. You should implement the methods (indicated with TODO), that performs forward and backward for an entire batch. Note, the idea is to use matrix multiplications, and not running standard loops over the instances in the batch."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kLdNoCt58qg5"
      },
      "outputs": [],
      "source": [
        "class MyNN:\n",
        "  def __init__(self, learning_rate, layer_sizes):\n",
        "    '''\n",
        "    learning_rate - the learning to use in backward\n",
        "    layer_sizes - a list of numbers, each number repreents the nuber of neurons\n",
        "                  to have in every layer. Therfore, the length of the list\n",
        "                  represents the number layers this network has.\n",
        "    '''\n",
        "    self.learning_rate = learning_rate\n",
        "\n",
        "    # layer_sizes: A list like [3, 5, 1] meaning:\n",
        "      # Input layer with 3 neurons\n",
        "      # hidden layer with 5 neurons\n",
        "      # Output layer with 1 neuron\n",
        "\n",
        "    self.layer_sizes = layer_sizes\n",
        "\n",
        "\n",
        "    self.model_params = {}\n",
        "    self.memory = {}\n",
        "    self.grads = {}\n",
        "\n",
        "    # Initializing weights\n",
        "    for layer_index in range(len(layer_sizes) - 1):\n",
        "      W_input = layer_sizes[layer_index + 1]\n",
        "      W_output = layer_sizes[layer_index]\n",
        "      self.model_params['W_' + str(layer_index + 1)] = np.random.randn(W_input, W_output) * 0.1\n",
        "      self.model_params['b_' + str(layer_index + 1)] = np.random.randn(W_input) * 0.1\n",
        "\n",
        "  # performs forward propagation through the neural network for a single input instance.\n",
        "  # This function takes a single input x (a 1D NumPy array) and returns the final output (i.e., prediction).\n",
        "  def forward_single_instance(self, x):\n",
        "    a_i_1 = x                                                     # a_i_1 is the activation from the previous layer (starting with input x).\n",
        "    self.memory['a_0'] = x                                        # The input is saved in self.memory for use during backpropagation\n",
        "    for layer_index in range(len(self.layer_sizes) - 1):          # Loop over layers - Iterates through all weight layers (i.e., from input → hidden → output).\n",
        "\n",
        "      # If layer_sizes = [3, 5, 1], the loop runs twice:\n",
        "      #   Layer 1: from input (3) to hidden (5)\n",
        "      #   Layer 2: from hidden (5) to output (1)\n",
        "\n",
        "      # Retrieves the weight matrix W_i and bias vector b_i for the current layer:\n",
        "      W_i = self.model_params['W_' + str(layer_index + 1)]\n",
        "      b_i = self.model_params['b_' + str(layer_index + 1)]\n",
        "\n",
        "      # Linear transformation - Zi = W_i * a_i_1 + b_i\n",
        "      # a_i_1 is the activation from the previous layer (starting with input x).\n",
        "      # W_i is the weight matrix for the current layer.\n",
        "      # b_i is the bias vector for the current layer.\n",
        "      # The dot product of W_i and a_i_1 is computed, and the bias b_i is added to it.\n",
        "      z_i = np.dot(W_i, a_i_1) + b_i\n",
        "      # Activation function - Ai = sigmoid(Zi)\n",
        "      # The sigmoid activation function is applied to the linear transformation result (z_i).\n",
        "      # The sigmoid function is defined as: sigmoid(z) = 1 / (1 + exp(-z))\n",
        "      # This function squashes the output to a range between 0 and 1.\n",
        "      # The result is stored in a_i, which represents the activation of the current layer.\n",
        "      a_i = 1/(1+np.exp(-z_i))\n",
        "\n",
        "      # The activation a_i is saved in self.memory for use during backpropagation.\n",
        "      self.memory['a_' + str(layer_index + 1)] = a_i\n",
        "\n",
        "      # The activation a_i becomes the input for the next layer (i.e., a_i_1 for the next iteration).\n",
        "      # \"a_i_1\" means a_{i-1}, i.e., activation from the previous layer\n",
        "      a_i_1 = a_i\n",
        "    return a_i_1\n",
        "\n",
        "  # binary cross-entropy loss (also called logistic loss) for a single prediction.\n",
        "  # y=1 y_hat=0 => true\n",
        "  # y=0 y_hat=1 => true\n",
        "  # y=y_hat     => false\n",
        "  def log_loss(self, y_hat, y):\n",
        "    '''\n",
        "    Logistic loss, assuming a single value in y_hat and y.\n",
        "    '''\n",
        "    m = y_hat[0]\n",
        "    cost = -y[0]*np.log(y_hat[0]) - (1 - y[0])*np.log(1 - y_hat[0])\n",
        "    return cost\n",
        "\n",
        "  # Performs backpropagation through the neural network for a single input instance.\n",
        "  # This function takes a single target value y (a 1D NumPy array) and computes the gradients.\n",
        "  # It computes gradients of the weights (and should also compute gradients of biases) to be used later in weight updates.\n",
        "  def backward_single_instance(self, y):\n",
        "    a_output = self.memory['a_' + str(len(self.layer_sizes) - 1)]           # The output of the last layer (i.e., the prediction). final output of the network = y_hat\n",
        "    dz = a_output - y               # The difference between the predicted output (a_output) and the true target value (y). derivative of loss w.r.t. \n",
        "\n",
        "    for layer_index in range(len(self.layer_sizes) - 1, 0, -1): #  Loop through layers in reverse order, if you have 3 layers ([3, 4, 1]), layer_index will be: 2 → 1\n",
        "      print(layer_index)\n",
        "      a_l_1 = self.memory['a_' + str(layer_index - 1)]  # the activation from the previous layer (i.e., a_{i-1}).\n",
        "      dW = np.dot(dz.reshape(-1, 1), a_l_1.reshape(1, -1))  # Gradient of the weights for the current layer.\n",
        "      self.grads['dW_' + str(layer_index)] = dW\n",
        "      W_l = self.model_params['W_' + str(layer_index)]\n",
        "      dz = (a_l_1 * (1 - a_l_1)).reshape(-1, 1) * np.dot(W_l.T, dz.reshape(-1, 1))\n",
        "      # TODO: calculate and memorize db as well.\n",
        "      db = dz.flatten() # Gradient of the biases for the current layer - flatten() returns a copy of an array collapsed into 1D (a flat vector).\n",
        "      self.grads['db_' + str(layer_index)] = db\n",
        "\n",
        "  # TODO: update weights with grads\n",
        "  def update(self):\n",
        "    \"\"\"\n",
        "    Updates weights and biases using the computed gradients and learning rate.\n",
        "    Uses gradient descent to update parameters.\n",
        "    \"\"\"\n",
        "    for layer_index in range(1, len(self.layer_sizes)):\n",
        "        # Get weight and bias gradients\n",
        "        dW = self.grads.get('dW_' + str(layer_index), 0)\n",
        "        db = self.grads.get('db_' + str(layer_index), 0)\n",
        "        \n",
        "        # Get current weights and biases\n",
        "        W = self.model_params['W_' + str(layer_index)]\n",
        "        b = self.model_params['b_' + str(layer_index)]\n",
        "        \n",
        "        # Update weights and biases using gradient descent\n",
        "        self.model_params['W_' + str(layer_index)] = W - self.learning_rate * dW\n",
        "        self.model_params['b_' + str(layer_index)] = b - self.learning_rate * db\n",
        "\n",
        "  # TODO: implement forward for a batch X.shape = (network_input_size, number_of_instance)\n",
        "  def forward_batch(self, X):\n",
        "    \"\"\"\n",
        "    Performs forward propagation for a batch of input instances.\n",
        "    \n",
        "    Parameters:\n",
        "    X -- Input data, shape: (network_input_size, number_of_instances)\n",
        "    \n",
        "    Returns:\n",
        "    batch_output -- Output predictions, shape: (output_size, number_of_instances)\n",
        "    \"\"\"\n",
        "    # Number of instances in the batch\n",
        "    number_of_instances = X.shape[1]\n",
        "    # Output size of the network (number of neurons in the last layer)\n",
        "    output_size = self.layer_sizes[-1]\n",
        "    \n",
        "    # Initialize output array\n",
        "    batch_output = np.zeros((output_size, number_of_instances))\n",
        "    \n",
        "    # Process each instance in the batch\n",
        "    for i in range(number_of_instances):\n",
        "        # Extract the i-th instance\n",
        "        x_i = X[:, i]\n",
        "        # Forward pass for the i-th instance\n",
        "        instance_output = self.forward_single_instance(x_i)\n",
        "        # Store the output of the i-th instance\n",
        "        batch_output[:, i] = instance_output\n",
        "    \n",
        "    return batch_output\n",
        "\n",
        "  # TODO: implement backward for a batch y.shape = (1, number_of_instance)\n",
        "  def backward_batch(self, y):\n",
        "    \"\"\"\n",
        "    Performs backpropagation for a batch of target values.\n",
        "    Computes gradients that are averaged across all instances.\n",
        "    \n",
        "    Parameters:\n",
        "    y -- Target values, shape: (1, number_of_instances)\n",
        "    \"\"\"\n",
        "    # Number of instances in the batch\n",
        "    number_of_instances = y.shape[1]\n",
        "    \n",
        "    # Initialize gradients storage for accumulation\n",
        "    grads_sum = {}\n",
        "    \n",
        "    # Process each instance in the batch\n",
        "    for i in range(number_of_instances):\n",
        "        # Extract the i-th target\n",
        "        y_i = y[:, i]\n",
        "        \n",
        "        # Backward pass for the i-th instance\n",
        "        self.backward_single_instance(y_i)\n",
        "        \n",
        "        # Accumulate gradients\n",
        "        for key in self.grads:\n",
        "            if key not in grads_sum:\n",
        "                grads_sum[key] = self.grads[key]\n",
        "            else:\n",
        "                grads_sum[key] += self.grads[key]\n",
        "    \n",
        "    # Average the accumulated gradients\n",
        "    for key in grads_sum:\n",
        "        self.grads[key] = grads_sum[key] / number_of_instances\n",
        "\n",
        "  # TODO: implement log_loss_batch, for a batch of instances\n",
        "  def log_loss_batch(self, y_hat, y):\n",
        "    \"\"\"\n",
        "    Computes the average binary cross-entropy loss for a batch of predictions.\n",
        "    \n",
        "    Parameters:\n",
        "    y_hat -- Predicted outputs, shape: (output_size, number_of_instances)\n",
        "    y -- Target values, shape: (output_size, number_of_instances)\n",
        "    \n",
        "    Returns:\n",
        "    cost -- Average loss across all instances\n",
        "    \"\"\"\n",
        "    # Number of instances in the batch\n",
        "    number_of_instances = y.shape[1]\n",
        "    \n",
        "    # Initialize total cost\n",
        "    total_cost = 0\n",
        "    \n",
        "    # Compute loss for each instance\n",
        "    for i in range(number_of_instances):\n",
        "        # Extract the i-th prediction and target\n",
        "        y_hat_i = y_hat[:, i].reshape(-1, 1)\n",
        "        y_i = y[:, i].reshape(-1, 1)\n",
        "        \n",
        "        # Compute loss for the i-th instance\n",
        "        instance_cost = self.log_loss(y_hat_i, y_i)\n",
        "        \n",
        "        # Accumulate total cost\n",
        "        total_cost += instance_cost\n",
        "    \n",
        "    # Compute average cost\n",
        "    cost = total_cost / number_of_instances\n",
        "    \n",
        "    return cost"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qib6W4QXO644"
      },
      "outputs": [],
      "source": [
        "nn = MyNN(0.01, [3, 2, 1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4nQR8QllPf_5"
      },
      "outputs": [],
      "source": [
        "nn.model_params"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VXiyn-yrPC6-"
      },
      "outputs": [],
      "source": [
        "x = np.random.randn(3)\n",
        "y = np.random.randn(1)\n",
        "\n",
        "y_hat = nn.forward_single_instance(x)\n",
        "print(y_hat)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k5M50i3plclj"
      },
      "outputs": [],
      "source": [
        "nn.backward_single_instance(y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CWnZB1YmYnIt"
      },
      "outputs": [],
      "source": [
        "def train(X, y, epochs, batch_size):\n",
        "  '''\n",
        "  Train procedure, please note the TODOs inside\n",
        "  '''\n",
        "  for e in range(1, epochs + 1):\n",
        "    epoch_loss = 0\n",
        "    # TODO: shuffle\n",
        "    batches = #... TODO: divide to batches\n",
        "    for X_b, y_b in batches:\n",
        "      y_hat = nn.forward_batch(X_b)\n",
        "      epoch_loss += nn.log_loss_batch(y_hat, y_b)\n",
        "      nn.backward_batch(y_b)\n",
        "      nn.update()\n",
        "    print(f'Epoch {e}, loss={epoch_loss/len(batches)}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cE1ydWlatkty"
      },
      "outputs": [],
      "source": [
        "# TODO: Make sure the following network trains properly\n",
        "\n",
        "nn = MyNN(0.001, [6, 4, 3, 1])\n",
        "\n",
        "X = np.random.randn(6, 100)\n",
        "y = np.random.randn(1, 100)\n",
        "batch_size = 8\n",
        "epochs = 2\n",
        "\n",
        "train(X, y, epochs, batch_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6dY4scUksulC"
      },
      "source": [
        "#TODO: train on an external dataset\n",
        "\n",
        "Train on the *hour.csv* file with a split of 75% training 10% validation and 15% for test.\n",
        "Use the following features from the data:\n",
        "\n",
        "* temp\n",
        "* atemp\n",
        "* hum\n",
        "* windspeed\n",
        "* weekday\n",
        "\n",
        "The response variable is, *success*\n",
        "\n",
        "The architecture of the network should be: [5, 40, 30, 10, 7, 5, 3, 1].\n",
        "\n",
        "Use batch_size=8, and train it for 100 epochs on the train set (based on the split as requested above).\n",
        "\n",
        "Then, plot train and validation loss per epoch."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZKAxdO2I1IGT"
      },
      "source": [
        "##  your code goes here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CHGoWEJk1K7r"
      },
      "source": [
        "###  Data Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e4Ra8vc_1NCE"
      },
      "outputs": [],
      "source": [
        "# TODO: Preprocess the bike sharing dataset ('hour.csv')\n",
        "# - Load the dataset from the provided hour.csv file\n",
        "# - Select the required features (temp, atemp, hum, windspeed, weekday)\n",
        "# - Extract the target variable (success)\n",
        "# - Normalize/standardize features if necessary\n",
        "# - Split the data into training (75%), validation (10%), and test (15%) sets\n",
        "# - Create DataLoader objects with batch_size=8"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "glp6Vz3B1TZo"
      },
      "source": [
        "### Model Training\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LXlKl21D1UGn"
      },
      "outputs": [],
      "source": [
        "# TODO: Train the neural network\n",
        "# - Implement the network with architecture [5, 40, 30, 10, 7, 5, 3, 1]\n",
        "# - Train for exactly 100 epochs on the training set\n",
        "# - Use batch_size=8 as specified\n",
        "# - Calculate and store train and validation loss for each epoch\n",
        "# - Track training progres"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cwBcg1yy1b-j"
      },
      "source": [
        "### Visualization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lqE6Jhxj1etd"
      },
      "outputs": [],
      "source": [
        "# TODO: Create visualizations of the learning process\n",
        "# - Plot the training loss per epoch\n",
        "# - Create additional relevant plots (validation loss, learning curves, etc.)\n",
        "# - Make sure all plots have proper labels, titles, and legends\n",
        "# - Add brief analysis of what the plots reveal about your model's performance"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xQIAgWhe1i_2"
      },
      "source": [
        "### Model Evaluation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IfquqstM1iIO"
      },
      "outputs": [],
      "source": [
        "# TODO: Evaluate model performance on the test set\n",
        "# - Calculate and report the loss on the test set\n",
        "# - Calculate and report the accuracy on the test set\n",
        "# - Compare test performance with training/validation performance\n",
        "# - Analyze model strengths and weaknesses\n",
        "# - Discuss any overfitting/underfitting issues observed"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
